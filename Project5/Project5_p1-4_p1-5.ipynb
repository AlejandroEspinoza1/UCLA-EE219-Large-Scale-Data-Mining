{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) {jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();} jQuery(\".highlight\").show();});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {jQuery(function() {var b = jQuery('<input type=\"button\" value=\"Show/Hide codes\"/>'); b.click(function(){jQuery('.input_area').each(function(){jQuery(this).toggle();});}); jQuery('[id^=Project]').parent().append(b);});});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Requirements: scikit-learn\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Prevent codes being displayed when exporting to an HTML file\n",
    "import IPython.core.display as di\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) {jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();} jQuery(\".highlight\").show();});</script>', raw=True)\n",
    "\n",
    "# Dynamically add a button to show/hide codes\n",
    "di.display_html(\"<script>jQuery(function() {jQuery(function() {var b = jQuery('<input type=\\\"button\\\" value=\\\"Show/Hide codes\\\"/>'); b.click(function(){jQuery('.input_area').each(function(){jQuery(this).toggle();});}); jQuery('[id^=Project]').parent().append(b);});});</script>\", raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Popularity Prediction on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tweets_#gohawks.txt', 'tweets_#gopatriots.txt', 'tweets_#nfl.txt', 'tweets_#patriots.txt', 'tweets_#sb49.txt', 'tweets_#superbowl.txt']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "folder_path = \"./tweet_data\" \n",
    "files= os.listdir(folder_path) # all the file name in the folder\n",
    "all_tweets = {}\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for file in files: \n",
    "     if not os.path.isdir(file): # open file if it is not a folder\n",
    "        f = open(folder_path + \"/\" + file) # open a file\n",
    "        templist = []\n",
    "        key = file[7:-4]\n",
    "        for line in open(folder_path + \"/\" + file): \n",
    "            line = f.readline()\n",
    "            tweet = json.loads(line)\n",
    "            templist.append(tweet)\n",
    "        all_tweets.setdefault(key, list)\n",
    "        all_tweets[key] = templist\n",
    "        f.close()\n",
    "        print(\"done!\")\n",
    "print (len(all_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# statistics per hour\n",
    "# including 'Total number of retweets', 'Sum of the number of followers', 'Maximum number of followers' per hour\n",
    "import datetime, time\n",
    "import pytz\n",
    "\n",
    "def feature_extract_perhour(start_time, end_time, tweetfile): \n",
    "    # only consider a part of the data, not every tweets in the file\n",
    "    total_hours = (end_time - start_time) / 3600 + 1\n",
    "    # print(total_hours)\n",
    "    tweets_totalnum_perhour = [0 for i in range(total_hours)]\n",
    "    retweets_num_perhour = [0 for i in range(total_hours)]\n",
    "    followers_totalnum_perhour = [0 for i in range(total_hours)]\n",
    "    followers_maxnum_perhour = [0 for i in range(total_hours)]\n",
    "    time_of_day_perhour = [None for i in range(total_hours)]\n",
    "    timestamp_perhour = []\n",
    "    ts = start_time\n",
    "    for i in range(total_hours):\n",
    "        timestamp_perhour.append(ts)\n",
    "        ts += 3600\n",
    "    pst_tz = pytz.timezone('US/Pacific') \n",
    "    \n",
    "    for tweet in tweetfile: \n",
    "        tweet_time = tweet['citation_date']\n",
    "        if (tweet_time >= start_time and tweet_time < end_time + 3600):\n",
    "            #hour = int((tweet_time - po_time_min)/3600)\n",
    "            hour = int((tweet_time - start_time) / 3600)\n",
    "            tweets_totalnum_perhour[hour] += 1\n",
    "            retweets_num_perhour[hour] += tweet['tweet']['retweet_count']\n",
    "            followers_totalnum_perhour[hour] += tweet['author']['followers']\n",
    "            if (tweet['author']['followers'] >= followers_maxnum_perhour[hour]):\n",
    "                followers_maxnum_perhour[hour] = tweet['author']['followers']\n",
    "            if (time_of_day_perhour[hour] == None):\n",
    "                # make the earliest time in each interval as the feature time_of_the_day \n",
    "                earliest_date = datetime.datetime.fromtimestamp(tweet['citation_date'], pst_tz)\n",
    "                time_of_day_perhour[hour] = earliest_date.hour\n",
    "    feat_5 = [tweets_totalnum_perhour[0:-1], retweets_num_perhour[0:-1], followers_totalnum_perhour[0:-1],\\\n",
    "              followers_maxnum_perhour[0:-1],time_of_day_perhour[0:-1]]\n",
    "    gt_y = tweets_totalnum_perhour[1:]\n",
    "    return timestamp_perhour[0:-1], feat_5, gt_y\n",
    "    # return index_timestamp, X_features, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(start_time, end_time, tweetfile): \n",
    "\n",
    "    total_hours = (end_time - start_time) / 3600 + 1\n",
    "    # other features:\n",
    "    ori_author_followers = [0 for i in range(total_hours)] # total number of followers of the original author\n",
    "    favorited_num = [0 for i in range(total_hours)] # total number of times of favorited, so many zeros\n",
    "    impressions_num = [0 for i in range(total_hours)]\n",
    "    avg_ranking_score = [0 for i in range(total_hours)]\n",
    "    user_mentions = [0 for i in range(total_hours)]\n",
    "    url_count = [0 for i in range(total_hours)]\n",
    "    unique_author_set = [set() for i in range(total_hours)] # save unique author \n",
    "    \n",
    "    # extract five basic features\n",
    "    index, feat_5, gt_y = feature_extract_perhour(start_time, end_time, tweetfile)\n",
    "\n",
    "    # extract other features\n",
    "    for tweet in tweetfile:\n",
    "        tweet_time = tweet['citation_date']\n",
    "        if (tweet_time >= start_time and tweet_time < end_time +3600):\n",
    "            hour = int((tweet_time - start_time) / 3600)\n",
    "            ori_author_followers[hour] += tweet['original_author']['followers']\n",
    "            favorited_num[hour] += tweet['tweet']['favorite_count']\n",
    "            user_mentions[hour] += len(tweet['tweet']['entities']['user_mentions'])\n",
    "            url_count[hour] += len(tweet['tweet']['entities']['urls'])\n",
    "            unique_author_set[hour].add(tweet['author']['nick'])\n",
    "            impressions_num[hour] += tweet['metrics']['impressions']\n",
    "            avg_ranking_score[hour] += tweet['metrics']['ranking_score'] # need to do average later\n",
    "            \n",
    "    total_tweets = feat_5[0]\n",
    "    for i in range(0, len(total_tweets)):\n",
    "        if(total_tweets[i] != 0):\n",
    "            avg_ranking_score[i] = avg_ranking_score[i] / total_tweets[i]\n",
    "    \n",
    "    unique_author_count =  [len(val) for val in unique_author_set]  # number of unique authors\n",
    "    feat_extra = [ori_author_followers[0:-1], favorited_num[0:-1], \\\n",
    "                  user_mentions[0:-1], url_count[0:-1], unique_author_count[0:-1],\\\n",
    "                  impressions_num[0:-1],avg_ranking_score[0:-1]]\n",
    "\n",
    "    feat_all = feat_5 + feat_extra\n",
    "\n",
    "    return index, feat_all, gt_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pst_tz = pytz.timezone('US/Pacific') \n",
    "round_timestamp_to_hour = lambda t: int(time.mktime(datetime.datetime.fromtimestamp(t, pst_tz).replace(microsecond=0,second=0,minute=0).timetuple()))\n",
    "\n",
    "mins = {}\n",
    "maxs = {}\n",
    "for key in all_tweets:\n",
    "    tmp_min = all_tweets[key][0]['citation_date']\n",
    "    tmp_max = 0\n",
    "    for tweet in all_tweets[key]:\n",
    "        tmp_min = min(tmp_min, tweet['citation_date'])\n",
    "        tmp_max = max(tmp_max, tweet['citation_date'])\n",
    "    mins[key] = round_timestamp_to_hour(tmp_min)\n",
    "    maxs[key] = round_timestamp_to_hour(tmp_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def features_transform(x):\n",
    "    # turn list of list (all features) into 2-D matrix: each row represents one \n",
    "    # reocrd(statistics for the one-hour interval), and add \"1\" to each row as bias\n",
    "    # deal with the intervals without tweets, set it as the value of previous interval plus 1\n",
    "    for i in range(len(x[4])):\n",
    "        if x[4][i] == None:\n",
    "            x[4][i] = (x[4][i - 1] + 1) % 24\n",
    "    X = np.array(x)\n",
    "    X = np.transpose(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "indexs, Xs, ys = [], [], []\n",
    "for key in all_tweets:\n",
    "    index, X, y = extract_features(mins[key], maxs[key], all_tweets[key])\n",
    "    X = features_transform(X)\n",
    "    y = np.array(y)\n",
    "    indexs.append(index)\n",
    "    Xs.append(X)\n",
    "    ys.append(y)\n",
    "\n",
    "pickle.dump(indexs, open(\"indexs.txt\", \"w\"))\n",
    "pickle.dump(Xs, open(\"Xs.txt\", \"w\"))\n",
    "pickle.dump(ys, open(\"ys.txt\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1.4 (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time1 = int(time.mktime(datetime.datetime(2015, 2, 1, 8, 0, 0, 0, pst_tz).timetuple()))\n",
    "time2 = int(time.mktime(datetime.datetime(2015, 2, 1, 20, 0, 0, 0, pst_tz).timetuple()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Three models with 10-fold CV\n",
    "def three_models(X, y):\n",
    "    XX = sm.add_constant(X)\n",
    "    kf = KFold(n_splits=10)\n",
    "    rmses = [0.0, 0.0, 0.0]\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        XX_train, XX_test = XX[train_index], XX[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # linear model\n",
    "        model = sm.regression.linear_model.OLS(y_train, XX_train)\n",
    "        results = model.fit()\n",
    "        prediction = results.predict(XX_test)\n",
    "        rmses[0] += math.sqrt(np.mean((y_test - prediction) ** 2))\n",
    "        # KNN\n",
    "        knn = KNeighborsRegressor(n_neighbors=10, n_jobs=-1)\n",
    "        knn.fit(X_train, y_train)\n",
    "        prediction = knn.predict(X_test)\n",
    "        rmses[1] += math.sqrt(np.mean((y_test - prediction) ** 2))\n",
    "        # Random Forest\n",
    "        regr = RandomForestRegressor(n_jobs=-1)\n",
    "        regr.fit(X_train, y_train)\n",
    "        prediction = regr.predict(X_test)\n",
    "        rmses[2] += math.sqrt(np.mean((y_test - prediction) ** 2))\n",
    "        \n",
    "    return [rmse / kf.get_n_splits() for rmse in rmses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexs = pickle.load(open(\"indexs.txt\", \"r\"))\n",
    "Xs = pickle.load(open(\"Xs.txt\", \"r\"))\n",
    "ys = pickle.load(open(\"ys.txt\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#superbowl': [[619.0927559597669, 708.3330926087743, 601.2628847873727], [3211734.0183889656, 97051.21956256071, 62291.24750562848], [897.6026597678998, 436.3887982133343, 345.958304089594]], '#gohawks': [[1799.157413664255, 460.26660639333005, 439.7710467522843], [56243.01385213109, 3434.841477502307, 2564.205422485475], [11645.93581350966, 44.91700588898525, 37.976714518638296]], '#sb49': [[80.00425575213788, 109.44365859347195, 87.28732759151542], [906625.1719303888, 35314.632388853155, 24039.573214589185], [304.5990076060568, 324.8604096411933, 166.0133866828014]], '#nfl': [[215.3847328445482, 249.6460569616193, 230.3008494629621], [49412.59832187189, 4146.565595663316, 3043.2066758320802], [247.59590266110757, 195.5082185381349, 182.49507161364625]], '#patriots': [[892.275788744394, 512.2530887853854, 515.9094412559979], [331059.905450379, 14867.864908549114, 14958.434509048733], [445.81607184709355, 167.99246822427227, 157.10411794466108]], '#gopatriots': [[36.738081925075036, 38.50211027126177, 27.077763907821083], [18121.076804419386, 1363.9612957122606, 1149.944360768011], [420.90542421625366, 5.5300862529452885, 4.513461699397576]]}\n"
     ]
    }
   ],
   "source": [
    "rmses = {}\n",
    "for i in range(len(indexs)):\n",
    "    key = files[i][7:-4]\n",
    "    rmses[key] = []\n",
    "    idx1, idx2 = indexs[i].index(time1), indexs[i].index(time2)\n",
    "    rmses[key].append(three_models(Xs[i][:idx1], ys[i][:idx1]))\n",
    "    rmses[key].append(three_models(Xs[i][idx1:idx2], ys[i][idx1:idx2]))\n",
    "    rmses[key].append(three_models(Xs[i][idx2:], ys[i][idx2:]))\n",
    "\n",
    "print(rmses)\n",
    "pickle.dump(rmses, open(\"rmses.txt\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Part 1.4 (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# aggregate through all tags\n",
    "X_train, y_train = [], []\n",
    "vector_X, vector_y = [[],[],[]], [[],[],[]]\n",
    "for i in range(len(indexs)):\n",
    "    idx1, idx2 = indexs[i].index(time1), indexs[i].index(time2)\n",
    "    vector_X[0].append(Xs[i][:idx1])\n",
    "    vector_y[0].append(ys[i][:idx1])\n",
    "    vector_X[1].append(Xs[i][idx1:idx2])\n",
    "    vector_y[1].append(ys[i][idx1:idx2])\n",
    "    vector_X[2].append(Xs[i][idx2:])\n",
    "    vector_y[2].append(ys[i][idx2:])\n",
    "\n",
    "for i in range(3):\n",
    "    X_train.append(np.concatenate(vector_X[i]))\n",
    "    y_train.append(np.concatenate(vector_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[259.80248584566283, 17028.236340002222, 85.73163860168893]\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(n_jobs=-1)\n",
    "rmses_all = []\n",
    "# period 1\n",
    "regr.fit(X_train[0], y_train[0])\n",
    "prediction = regr.predict(X_train[0])\n",
    "rmses_all.append(math.sqrt(np.mean((y_train[0] - prediction) ** 2)))\n",
    "# period 2\n",
    "regr.fit(X_train[1], y_train[1])\n",
    "prediction = regr.predict(X_train[1])\n",
    "rmses_all.append(math.sqrt(np.mean((y_train[1] - prediction) ** 2)))\n",
    "# period 3\n",
    "regr.fit(X_train[2], y_train[2])\n",
    "prediction = regr.predict(X_train[2])\n",
    "rmses_all.append(math.sqrt(np.mean((y_train[2] - prediction) ** 2)))\n",
    "print(rmses_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform data to 5-hour time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using citation date, so do not split the periods (https://piazza.com/class/jcifzza0hzs2f3?cid=310)\n",
    "vectors_X = []\n",
    "vectors_y = []    \n",
    "for i in range(len(Xs)):\n",
    "    for j in range(len(Xs[i]) - 5):\n",
    "        vectors_X.append(np.concatenate([Xs[i][j], Xs[i][j+1], Xs[i][j+2], Xs[i][j+3], Xs[i][j+4]]))\n",
    "    vectors_y.extend(ys[i][5:])\n",
    "\n",
    "X_new = np.vstack(vectors_X)\n",
    "delete_index = [ 1, 2, 4, 6, 9,10,11,\n",
    "                13,14,16,18,21,22,23,\n",
    "                25,26,28,30,33,34,35,\n",
    "                37,38,40,42,45,46,47,\n",
    "                49,50,52,54,57,58,59]\n",
    "X_new = np.delete(X_new, delete_index, 1) \n",
    "y_new = np.array(vectors_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length: 25\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(n_jobs=-1)\n",
    "regr.fit(X_new, y_new)\n",
    "print(\"Feature vector length: %d\" % len(X_new[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extraction on test data (only last 6 hours data based on citation date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_start_end_timestamp(tweetlist):\n",
    "    tmp_max = 0\n",
    "    for tweet in tweetlist:\n",
    "        tmp_max = max(tmp_max, tweet['citation_date'])\n",
    "\n",
    "    tmp_max = round_timestamp_to_hour(tmp_max)\n",
    "    return tmp_max - 3600*5, tmp_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: sample10_period3.txt\n",
      "Truth: 61\n",
      "Predict: 50\n",
      "File: sample1_period1.txt\n",
      "Truth: 1\n",
      "Predict: 15\n",
      "File: sample2_period2.txt\n",
      "Truth: 4\n",
      "Predict: 12\n",
      "File: sample3_period3.txt\n",
      "Truth: 523\n",
      "Predict: 632\n",
      "File: sample4_period1.txt\n",
      "Truth: 201\n",
      "Predict: 209\n",
      "File: sample5_period1.txt\n",
      "Truth: 1\n",
      "Predict: 2\n",
      "File: sample6_period2.txt\n",
      "Truth: 14\n",
      "Predict: 51\n",
      "File: sample7_period3.txt\n",
      "Truth: 120\n",
      "Predict: 54\n",
      "File: sample8_period1.txt\n",
      "Truth: 11\n",
      "Predict: 84\n",
      "File: sample9_period2.txt\n",
      "Truth: 1\n",
      "Predict: 0\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./test_data\" \n",
    "files= os.listdir(folder_path)\n",
    "for file in files: \n",
    "     if not os.path.isdir(file):\n",
    "        print(\"File: %s\" % file)\n",
    "        tweetlist = []\n",
    "        with open(folder_path + \"/\" + file) as f: # open a file\n",
    "            for line in f.readlines(): \n",
    "                tweet = json.loads(line)\n",
    "                tweetlist.append(tweet)\n",
    "\n",
    "        start_time, end_time = get_start_end_timestamp(tweetlist)\n",
    "        index, X, y = extract_features(start_time, end_time, tweetlist)\n",
    "\n",
    "        X[4][0] = datetime.datetime.fromtimestamp(start_time, pst_tz).hour\n",
    "        for i in range(1, len(X[4])):\n",
    "            X[4][i] = (X[4][i - 1] + 1) % 24\n",
    "\n",
    "        X = np.array(X)\n",
    "        X = np.transpose(X).astype('int')\n",
    "        X = np.delete(X, [1,2,4,6,9,10,11], 1)\n",
    "        y = np.array(y)\n",
    "\n",
    "        y_test = y[-1]\n",
    "        X_test = np.concatenate([X[0], X[1], X[2], X[3], X[4]])\n",
    "        X_test = np.vstack([X_test])\n",
    "\n",
    "        print(\"Truth: %d\" % y_test)\n",
    "        print(\"Predict: %d\" % regr.predict(X_test)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
